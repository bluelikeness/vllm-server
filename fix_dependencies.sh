#!/bin/bash

# vLLM ì˜ì¡´ì„± ì¶©ëŒ í•´ê²° ìŠ¤í¬ë¦½íŠ¸
echo "ğŸ”§ vLLM ì˜ì¡´ì„± ì¶©ëŒ í•´ê²°"
echo "========================"

cd ~/vllm_server

# ê¸°ì¡´ íŒ¨í‚¤ì§€ ì •ë¦¬
echo "ğŸ§¹ ê¸°ì¡´ íŒ¨í‚¤ì§€ ì •ë¦¬..."
pip uninstall -y torch torchvision vllm transformers 2>/dev/null || true

# ìºì‹œ ì •ë¦¬
echo "ğŸ—‘ï¸ pip ìºì‹œ ì •ë¦¬..."
pip cache purge

# ë‹¨ê³„ë³„ ì¬ì„¤ì¹˜
echo "ğŸ“¦ ë‹¨ê³„ë³„ ì¬ì„¤ì¹˜..."

echo "  1ï¸âƒ£ PyTorch ì„¤ì¹˜ (CUDA 118)..."
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

echo "  2ï¸âƒ£ Transformers ì„¤ì¹˜..."
pip install transformers

echo "  3ï¸âƒ£ ê¸°ë³¸ ì˜ì¡´ì„± ì„¤ì¹˜..."
pip install fastapi uvicorn Pillow python-dotenv requests

echo "  4ï¸âƒ£ Ray ì„¤ì¹˜..."
pip install "ray[default]"

echo "  5ï¸âƒ£ vLLM ì„¤ì¹˜..."
pip install vllm

echo "âœ… ì„¤ì¹˜ ì™„ë£Œ!"

# í™•ì¸
echo "ğŸ” ì„¤ì¹˜ í™•ì¸..."
python -c "
import torch
import vllm
import transformers
print(f'PyTorch: {torch.__version__}')
print(f'vLLM: {vllm.__version__}')
print(f'Transformers: {transformers.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
"
