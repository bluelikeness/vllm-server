import os
import time
import logging
from typing import Any, Dict, List, Optional, Tuple
import re
from PIL import Image

import torch
from vllm import AsyncLLMEngine, AsyncEngineArgs
from vllm.sampling_params import SamplingParams
from vllm.utils import random_uuid
from vllm.inputs import TextPrompt

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [GPU] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('/tmp/vllm_engine.log')
    ]
)
logger = logging.getLogger(__name__)


MULTIMODAL_AVAILABLE = True
try:
    from vllm.multimodal import MultiModalDataDict  # noqa: F401
    MULTIMODAL_AVAILABLE = True
except Exception:
    try:
        from vllm.multimodal import MultiModalData  # noqa: F401
        MULTIMODAL_AVAILABLE = True
    except Exception:
        MULTIMODAL_AVAILABLE = False


vllm_engine: Optional[AsyncLLMEngine] = None
engine_config: Dict[str, Any] = {}


async def initialize_vllm_engine() -> bool:
    global vllm_engine, engine_config
    init_start = time.time()

    logger.info("ğŸš€ vLLM ì—”ì§„ ì´ˆê¸°í™” ì‹œì‘")

    try:
        # ì•ˆì „ ê¸°ë³¸ê°’: í…ì„œ ë³‘ë ¬ ë¯¸ì§€ì • ì‹œ 1ë¡œ ë™ì‘
        tensor_parallel_size = 1
        load_mode = os.getenv("VLLM_LOAD_MODE", "fp16").lower()

        # Determine mode key for presets
        mode_key = "FP16"
        if load_mode in ("int4", "4bit"):
            mode_key = "INT4"
        elif load_mode in ("int8", "8bit"):
            mode_key = "INT8"
        elif load_mode in ("bf16", "bfloat16"):
            mode_key = "BF16"

        def pick_env(name: str, default: Optional[str] = None) -> Optional[str]:
            """Pick value with priority: MODE_NAME > generic NAME > default.
            Example: name='MAX_MODEL_LEN' checks f"{mode_key}_{name}", then f"VLLM_{name}".
            """
            mode_var = f"{mode_key}_{name}"
            generic_var = f"VLLM_{name}"
            val = os.getenv(mode_var)
            if val is not None and str(val).strip() != "":
                return val
            val = os.getenv(generic_var)
            if val is not None and str(val).strip() != "":
                return val
            return default

        quant_method_env = pick_env("QUANTIZATION_METHOD", "").lower()
        kv_cache_dtype = pick_env("KV_CACHE_DTYPE", "")
        model_name = os.getenv("MODEL_NAME", "Qwen/Qwen2.5-VL-32B-Instruct")

        # LoRA ì–´ëŒ‘í„° ì„¤ì • ì½ê¸°
        lora_adapters = os.getenv("LORA_ADAPTERS", "").strip()
        lora_adapter_names = os.getenv("LORA_ADAPTER_NAMES", "").strip()
        default_lora_adapter = os.getenv("DEFAULT_LORA_ADAPTER", "").strip()

        logger.info(f"ğŸ“‚ ëª¨ë¸: {model_name}")
        logger.info(f"âš™ï¸ ë¡œë“œ ëª¨ë“œ: {load_mode}")
        logger.info(f"ğŸ”§ ì–‘ìí™” ë°©ë²•: {quant_method_env or 'ì—†ìŒ'}")
        logger.info(f"ğŸ’¾ KV ìºì‹œ íƒ€ì…: {kv_cache_dtype or 'auto'}")

        # LoRA ì–´ëŒ‘í„° ì •ë³´ ë¡œê¹…
        if lora_adapters:
            adapter_list = [a.strip() for a in lora_adapters.split(',') if a.strip()]
            name_list = [n.strip() for n in lora_adapter_names.split(',') if n.strip()] if lora_adapter_names else []
            logger.info(f"ğŸ¯ LoRA ì–´ëŒ‘í„°: {len(adapter_list)}ê°œ ê°ì§€")
            for i, adapter_path in enumerate(adapter_list):
                adapter_name = name_list[i] if i < len(name_list) else f"adapter_{i+1}"
                logger.info(f"  - {adapter_name}: {adapter_path}")
            if default_lora_adapter:
                logger.info(f"ğŸŒŸ ê¸°ë³¸ ì–´ëŒ‘í„°: {default_lora_adapter}")
        else:
            logger.info("ğŸ¯ LoRA ì–´ëŒ‘í„°: ì—†ìŒ (ë² ì´ìŠ¤ ëª¨ë¸ë§Œ ì‚¬ìš©)")

        quantization = None
        # KV ìºì‹œëŠ” ëª¨ë“  ëª¨ë“œì—ì„œ ì ìš© ê°€ëŠ¥í•˜ë„ë¡ ê³µí†µ ì²˜ë¦¬
        kv_cache_dtype_setting = "auto"
        if kv_cache_dtype and kv_cache_dtype.lower() in ("fp8", "int8"):
            kv_cache_dtype_setting = kv_cache_dtype.lower()

        if load_mode in ("int4", "4bit"):
            # 4bit ì–‘ìí™” ì„¤ì •
            if quant_method_env in ("awq", "gptq", "bitsandbytes", "bnb"):
                quantization = "bitsandbytes" if quant_method_env in ("bitsandbytes", "bnb") else quant_method_env
            else:
                quantization = "bitsandbytes"  # ê¸°ë³¸ê°’ì„ bitsandbytesë¡œ ë³€ê²½

            logger.info(f"ğŸ”¢ ì ìš©ëœ ì–‘ìí™”: {quantization}")
            logger.info(f"ğŸ—„ï¸ KV ìºì‹œ ìµœì í™”: {kv_cache_dtype_setting}")
        elif load_mode in ("int8", "8bit"):
            # 8bit ì–‘ìí™” ì„¤ì •
            if quant_method_env in ("bitsandbytes", "bnb", "gptq", "awq"):
                # INT8 ê¸°ë³¸ì€ bitsandbytes ê¶Œì¥
                quantization = "bitsandbytes" if quant_method_env in ("", "bitsandbytes", "bnb") else quant_method_env
            else:
                quantization = "bitsandbytes"

            logger.info(f"ğŸ”¢ ì ìš©ëœ ì–‘ìí™”: {quantization}")
            logger.info(f"ğŸ—„ï¸ KV ìºì‹œ ìµœì í™”: {kv_cache_dtype_setting}")

        env_max_len = pick_env("MAX_MODEL_LEN")
        env_gpu_util = pick_env("GPU_MEMORY_UTILIZATION")

        # 4bit ëª¨ë“œì—ì„œ ë” í° ì»¨í…ìŠ¤íŠ¸ì™€ ë” ë§ì€ ì‹œí€€ìŠ¤ ì²˜ë¦¬ ê°€ëŠ¥
        if load_mode in ("int4", "4bit"):
            default_max_len = 12288  # 4bitì—ì„œ ë” í° ì»¨í…ìŠ¤íŠ¸
            default_gpu_util = 0.85   # 4bitì—ì„œ ì•ˆì •ì  ë©”ëª¨ë¦¬ ì‚¬ìš©
        elif load_mode in ("int8", "8bit"):
            default_max_len = 10240  # 8bitì—ì„œ ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸
            default_gpu_util = 0.90   # 8bitì—ì„œ ë‹¤ì†Œ ë†’ì€ í™œìš©ë¥  ê°€ëŠ¥
        else:  # fp16/bf16
            default_max_len = 8192    # FP16/BF16 ê¸°ë³¸ê°’
            default_gpu_util = 0.92   # FP16/BF16ì—ì„œ ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

        chosen_max_len = int(env_max_len) if env_max_len else default_max_len
        chosen_gpu_util = float(env_gpu_util) if env_gpu_util else default_gpu_util

        # 4bit ëª¨ë“œì—ì„œ ì‹œí€€ìŠ¤ ìˆ˜ ìµœì í™”
        if load_mode in ("int4", "4bit"):
            default_max_seqs = 48  # 4bitì—ì„œ ë” ë§ì€ ë™ì‹œ ì²˜ë¦¬
            default_block_size = 32  # ë” í° ë¸”ë¡ í¬ê¸°
        elif load_mode in ("int8", "8bit"):
            default_max_seqs = 36   # 8bitì—ì„œ ë™ì‹œ ì²˜ë¦¬ í™•ëŒ€
            default_block_size = 32
        else:  # fp16/bf16
            default_max_seqs = 24   # FP16/BF16 ê¸°ë³¸ê°’
            default_block_size = 32

        max_num_seqs = int(pick_env("MAX_NUM_SEQS", str(default_max_seqs)))
        block_size = int(pick_env("BLOCK_SIZE", str(default_block_size)))
        swap_space = int(pick_env("SWAP_SPACE", "4"))

        # Tensor parallel size (support mode preset or generic)
        tensor_parallel_size = int(
            pick_env("TENSOR_PARALLEL_SIZE", str(int(os.getenv("VLLM_TENSOR_PARALLEL_SIZE", "1"))))
        )

        logger.info(f"ğŸ“ ìµœëŒ€ ëª¨ë¸ ê¸¸ì´: {chosen_max_len}")
        logger.info(f"ğŸ¯ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {chosen_gpu_util}")
        logger.info(f"ğŸ”— í…ì„œ ë³‘ë ¬ í¬ê¸°: {tensor_parallel_size}")
        logger.info(f"ğŸ“Š ìµœëŒ€ ì‹œí€€ìŠ¤ ìˆ˜: {max_num_seqs}")
        logger.info(f"ğŸ§± ë¸”ë¡ í¬ê¸°: {block_size}")
        logger.info(f"ğŸ’½ ìŠ¤ì™‘ ê³µê°„: {swap_space}GB")

        # trust_remote_code ì„¤ì • (.env ì œì–´ ê°€ëŠ¥; ê¸°ë³¸ True)
        trc_val = os.getenv("VLLM_TRUST_REMOTE_CODE", "1").strip().lower()
        trust_remote_code = trc_val in ("1", "true", "yes", "y")

        # vLLM ì—”ì§„ ì¸ì êµ¬ì„±
        engine_args_dict = {
            "model": model_name,
            "tensor_parallel_size": tensor_parallel_size,
            "max_model_len": chosen_max_len,
            "max_num_seqs": max_num_seqs,
            "block_size": block_size,
            "swap_space": swap_space,
            "gpu_memory_utilization": chosen_gpu_util,
            "trust_remote_code": trust_remote_code,
            "enforce_eager": False,
            "limit_mm_per_prompt": {"image": 1} if MULTIMODAL_AVAILABLE else {"image": 0},
        }

        # LoRA ì–´ëŒ‘í„° ì„¤ì • ì¶”ê°€
        lora_paths_list: List[str] = []
        if lora_adapters:
            adapter_list = [a.strip() for a in lora_adapters.split(',') if a.strip()]
            name_list = [n.strip() for n in lora_adapter_names.split(',') if n.strip()] if lora_adapter_names else []

            if adapter_list:
                # LoRA ì–´ëŒ‘í„° ê²½ë¡œë“¤ ì„¤ì •
                engine_args_dict["enable_lora"] = True
                engine_args_dict["lora_modules"] = []
                lora_paths_list = []

                for i, adapter_path in enumerate(adapter_list):
                    adapter_name = name_list[i] if i < len(name_list) else f"adapter_{i+1}"
                    engine_args_dict["lora_modules"].append({
                        "name": adapter_name,
                        "path": adapter_path
                    })
                    lora_paths_list.append(adapter_path)

                # ê¸°ë³¸ ì–´ëŒ‘í„° ì„¤ì •
                if default_lora_adapter and default_lora_adapter in [n.get("name") for n in engine_args_dict["lora_modules"]]:
                    engine_args_dict["default_lora_adapter"] = default_lora_adapter

                logger.info(f"âœ… LoRA ì§€ì› í™œì„±í™”: {len(adapter_list)}ê°œ ì–´ëŒ‘í„°")

        # ì–‘ìí™” ì„¤ì • ì¶”ê°€
        if quantization:
            engine_args_dict["quantization"] = quantization

        # KV ìºì‹œ dtype ì„¤ì • (ëª¨ë“  ëª¨ë“œì—ì„œ ì ìš©; vLLM ë²„ì „ì— ë”°ë¼ ì§€ì› ì—¬ë¶€ í™•ì¸)
        if kv_cache_dtype_setting != "auto":
            try:
                engine_args_dict["kv_cache_dtype"] = kv_cache_dtype_setting
                logger.info(f"ğŸ’¾ KV ìºì‹œ íƒ€ì… ì„¤ì •: {kv_cache_dtype_setting}")
            except Exception as e:
                logger.warning(f"âš ï¸ KV ìºì‹œ íƒ€ì… ì„¤ì • ì‹¤íŒ¨ (vLLM ë²„ì „ í˜¸í™˜ì„±): {e}")

        # vLLM ë²„ì „ í˜¸í™˜: ì§€ì›ë˜ì§€ ì•ŠëŠ” í‚¤ê°€ ìˆìœ¼ë©´ ì œê±°/ëŒ€ì²´í•˜ë©° ì¬ì‹œë„
        attempt_dict = dict(engine_args_dict)
        for _ in range(5):
            try:
                engine_args = AsyncEngineArgs(**attempt_dict)
                break
            except TypeError as e:
                msg = str(e)
                m = re.search(r"unexpected keyword argument '([^']+)'", msg)
                if not m:
                    raise
                bad_key = m.group(1)
                logger.warning(f"âš ï¸ AsyncEngineArgsì—ì„œ ì§€ì›ë˜ì§€ ì•ŠëŠ” ì¸ì ê°ì§€: {bad_key} â†’ ì œê±°/ëŒ€ì²´ í›„ ì¬ì‹œë„")
                # lora_modules ë¯¸ì§€ì›: lora_pathsë¡œ ëŒ€ì²´ ì‹œë„
                if bad_key == "lora_modules" and lora_paths_list:
                    attempt_dict.pop("lora_modules", None)
                    attempt_dict["lora_paths"] = list(lora_paths_list)
                else:
                    attempt_dict.pop(bad_key, None)
        else:
            # ë°˜ë³µ ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨ ì‹œ ë§ˆì§€ë§‰ìœ¼ë¡œ ì˜ˆì™¸ ë°œìƒ
            engine_args = AsyncEngineArgs(**attempt_dict)

        logger.info(f"ğŸ–¼ï¸ ë©€í‹°ëª¨ë‹¬ ì§€ì›: {'í™œì„±í™”' if MULTIMODAL_AVAILABLE else 'ë¹„í™œì„±í™”'}")
        logger.info(f"âš¡ ëª¨ë“œë³„ ìµœì í™”: {'4bit/8bit ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±' if load_mode in ('int4', '4bit', 'int8', '8bit') else 'FP16/BF16 ê³ ì„±ëŠ¥'}")
        logger.info(f"ğŸ’¾ KV ìºì‹œ ë°ì´í„° íƒ€ì… ì„¤ì •(ìš”ì²­): {kv_cache_dtype or 'auto'} â†’ ì ìš©: {kv_cache_dtype_setting}")

        logger.info("ğŸ”„ vLLM ì—”ì§„ ìƒì„± ì¤‘...")
        engine_create_start = time.time()

        try:
            vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)
            engine_create_time = time.time() - engine_create_start
            logger.info(f"âœ… vLLM ì—”ì§„ ìƒì„± ì™„ë£Œ - {engine_create_time:.2f}ì´ˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ ì²« ë²ˆì§¸ ì‹œë„ ì‹¤íŒ¨: {str(e)}")
            if "bitsandbytes" in str(e) or "quantizer" in str(e).lower():
                logger.info("ğŸ”„ ì–‘ìí™” ì—†ì´ ì¬ì‹œë„...")
                quantization = None
                engine_args.quantization = None
                engine_args.gpu_memory_utilization = float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.85"))
                vllm_engine = AsyncLLMEngine.from_engine_args(engine_args)
                logger.info("âœ… ì–‘ìí™” ì—†ì´ ì—”ì§„ ìƒì„± ì„±ê³µ")
            else:
                raise

        engine_config.update({
            "model": engine_args.model,
            "tensor_parallel_size": engine_args.tensor_parallel_size,
            "max_model_len": engine_args.max_model_len,
            "max_num_seqs": engine_args.max_num_seqs,
            "gpu_memory_utilization": engine_args.gpu_memory_utilization,
            "load_mode": load_mode,
            "quantization": quantization or "none",
            "kv_cache_dtype": kv_cache_dtype or None,
        })

        # GPU ìƒíƒœ ë¡œê¹…
        gpu_status = get_gpu_status()
        logger.info(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬: {gpu_status['memory_used']:.2f}GB / {gpu_status['memory_total']:.2f}GB ì‚¬ìš©")

        init_time = time.time() - init_start
        logger.info(f"ğŸ¯ vLLM ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ - ì´ {init_time:.2f}ì´ˆ")

        return True
    except Exception as e:
        init_time = time.time() - init_start
        logger.error(f"âŒ vLLM ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨ - {init_time:.2f}ì´ˆ: {e}")
        return False


async def get_vllm_stats() -> Dict[str, Any]:
    global vllm_engine
    if vllm_engine is None:
        return {"error": "vLLM ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
    return {
        "engine_status": "running",
        "pending_requests": 0,
        "running_requests": 0,
    }


def get_gpu_status() -> Dict[str, float]:
    try:
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated() / 1024 ** 3
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3
            return {"memory_used": round(memory_used, 2), "memory_total": round(memory_total, 2)}
        else:
            return {"memory_used": 0.0, "memory_total": 0.0}
    except Exception as e:
        print(f"GPU ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {e}")
        return {"memory_used": 0.0, "memory_total": 0.0}


async def generate_with_vllm(
    prompt: str,
    max_tokens: int = 512,
    temperature: float = 0.7,
    images: Optional[List[Image.Image]] = None,
    lora_adapter: Optional[str] = None,  # ğŸ†• ìš”ì²­ë³„ LoRA ì–´ëŒ‘í„° ì„ íƒ
) -> Tuple[str, Dict[str, Any]]:
    global vllm_engine
    if vllm_engine is None:
        raise RuntimeError("vLLM ì—”ì§„ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    request_id = random_uuid()[:8]  # ì§§ì€ ìš”ì²­ ID
    start_time = time.time()
    timings: Dict[str, Any] = {}
    original_prompt = prompt

    logger.info(f"ğŸ¯ [GPU-{request_id}] í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘")
    logger.info(f"ğŸ“ [GPU-{request_id}] í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)}ì")
    logger.info(f"ğŸ–¼ï¸ [GPU-{request_id}] ì´ë¯¸ì§€ í¬í•¨: {'ì˜ˆ' if images and len(images) > 0 else 'ì•„ë‹ˆì˜¤'}")
    logger.info(f"ğŸ›ï¸ [GPU-{request_id}] ìµœëŒ€ í† í°: {max_tokens}, ì˜¨ë„: {temperature}")

    # LoRA ì–´ëŒ‘í„° ì •ë³´ ë¡œê¹…
    if lora_adapter:
        logger.info(f"ğŸ¯ [GPU-{request_id}] ìš”ì²­ëœ LoRA ì–´ëŒ‘í„°: {lora_adapter}")
    else:
        default_adapter = os.getenv("DEFAULT_LORA_ADAPTER", "")
        if default_adapter:
            lora_adapter = default_adapter
            logger.info(f"ğŸŒŸ [GPU-{request_id}] ê¸°ë³¸ LoRA ì–´ëŒ‘í„° ì‚¬ìš©: {lora_adapter}")
        else:
            logger.info(f"ğŸ¯ [GPU-{request_id}] LoRA ì–´ëŒ‘í„°: ì‚¬ìš© ì•ˆí•¨ (ë² ì´ìŠ¤ ëª¨ë¸)")

    eff_tokens = max(1, int(min(max_tokens, int(os.getenv("MAX_TOKENS_CAP", "512")))))
    if eff_tokens != max_tokens:
        logger.info(f"âš™ï¸ [GPU-{request_id}] í† í° ìˆ˜ ì¡°ì •: {max_tokens} -> {eff_tokens}")

    sampling_params = SamplingParams(
        max_tokens=eff_tokens,
        temperature=temperature,
        top_p=0.9,
        repetition_penalty=1.05,
        stop_token_ids=[],
    )

    # ğŸ†• LoRA ì–´ëŒ‘í„°ê°€ ì§€ì •ëœ ê²½ìš° sampling_paramsì— ì¶”ê°€
    if lora_adapter:
        try:
            # vLLM 0.2.0+ ì—ì„œ ì§€ì›í•˜ëŠ” LoRA ìš”ì²­ë³„ ì§€ì •
            sampling_params.lora_request = lora_adapter
            logger.info(f"âœ… [GPU-{request_id}] LoRA ì–´ëŒ‘í„° ì„¤ì • ì™„ë£Œ: {lora_adapter}")
        except AttributeError:
            # êµ¬ë²„ì „ vLLMì—ì„œëŠ” ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            logger.warning(f"âš ï¸ [GPU-{request_id}] í˜„ì¬ vLLM ë²„ì „ì—ì„œ ìš”ì²­ë³„ LoRA ì§€ì • ë¯¸ì§€ì›")
        except Exception as e:
            logger.warning(f"âš ï¸ [GPU-{request_id}] LoRA ì–´ëŒ‘í„° ì„¤ì • ì‹¤íŒ¨: {e}")

    use_multimodal = False
    if images and MULTIMODAL_AVAILABLE:
        try:
            logger.info(f"ğŸ–¼ï¸ [GPU-{request_id}] ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì¤‘...")
            if "<|image_pad|>" not in prompt and "<|vision_start|>" not in prompt:
                prompt = f"<|vision_start|><|image_pad|><|vision_end|>\n{prompt}"
                logger.info(f"ğŸ“„ [GPU-{request_id}] ë¹„ì „ íƒœê·¸ ì¶”ê°€ë¨")

            # ì´ë¯¸ì§€ ì •ë³´ ë¡œê¹…
            if images[0]:
                img_size = images[0].size
                img_mode = images[0].mode
                logger.info(f"ğŸ–¼ï¸ [GPU-{request_id}] ì´ë¯¸ì§€ ì •ë³´: {img_size}, ëª¨ë“œ: {img_mode}")

            prompt = TextPrompt({"prompt": prompt, "multi_modal_data": {"image": images[0]} if images else {}})
            use_multimodal = True
            logger.info(f"âœ… [GPU-{request_id}] ë©€í‹°ëª¨ë‹¬ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"âš ï¸ [GPU-{request_id}] ë©€í‹°ëª¨ë‹¬ ì¤€ë¹„ ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ì „í™˜: {e}")
            prompt = original_prompt
            use_multimodal = False

    # GPU ìƒíƒœ ë¡œê¹…
    gpu_status = get_gpu_status()
    logger.info(f"ğŸ–¥ï¸ [GPU-{request_id}] ìƒì„± ì „ GPU ë©”ëª¨ë¦¬: {gpu_status['memory_used']:.2f}GB / {gpu_status['memory_total']:.2f}GB")

    t_gen_start = time.time()
    logger.info(f"ğŸš€ [GPU-{request_id}] vLLM ìƒì„± ì‹œì‘...")

    try:
        results_generator = vllm_engine.generate(prompt, sampling_params, request_id)
    except Exception as e:
        logger.error(f"âŒ [GPU-{request_id}] ìƒì„± ì‹œì‘ ì‹¤íŒ¨: {e}")
        if use_multimodal:
            logger.info(f"ğŸ”„ [GPU-{request_id}] í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ì¬ì‹œë„...")
            prompt = original_prompt
            results_generator = vllm_engine.generate(prompt, sampling_params, request_id)
        else:
            raise

    final_output = None
    try:
        logger.info(f"â³ [GPU-{request_id}] ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì¤‘...")
        async for request_output in results_generator:
            final_output = request_output
        logger.info(f"âœ… [GPU-{request_id}] ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ")
    except Exception as e:
        logger.error(f"âŒ [GPU-{request_id}] ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
        if use_multimodal:
            logger.info(f"ğŸ”„ [GPU-{request_id}] í…ìŠ¤íŠ¸ ëª¨ë“œë¡œ ì¬ì‹œë„...")
            prompt = original_prompt
            results_generator = vllm_engine.generate(prompt, sampling_params, request_id)
            async for request_output in results_generator:
                final_output = request_output
        else:
            raise

    timings["generation_ms"] = round((time.time() - t_gen_start) * 1000, 1)

    if final_output is None:
        logger.error(f"âŒ [GPU-{request_id}] ìƒì„± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        raise RuntimeError("ìƒì„± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")

    response_text = "".join(o.text for o in final_output.outputs)
    timings["total_ms"] = round((time.time() - start_time) * 1000, 1)
    timings["tokens_generated"] = len(final_output.outputs[0].token_ids) if final_output.outputs else 0

    # íƒ€ì… ì•ˆì „í•œ í† í°/ì´ˆ ê³„ì‚°
    generation_time_seconds = float(timings["generation_ms"]) / 1000.0 if timings.get("generation_ms", 0) > 0 else 0
    timings["tokens_per_second"] = round(
        float(timings["tokens_generated"]) / generation_time_seconds, 1
    ) if generation_time_seconds > 0 else 0

    # ìƒì„± ì™„ë£Œ ë¡œê¹…
    logger.info(f"ğŸ¯ [GPU-{request_id}] í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ")
    logger.info(f"â±ï¸ [GPU-{request_id}] ìƒì„± ì‹œê°„: {timings['generation_ms']}ms")
    logger.info(f"ğŸ“Š [GPU-{request_id}] ìƒì„± í† í°: {timings['tokens_generated']}ê°œ")
    logger.info(f"ğŸš€ [GPU-{request_id}] ì†ë„: {timings['tokens_per_second']} tokens/sec")
    logger.info(f"ğŸ“¤ [GPU-{request_id}] ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
    logger.info(f"ğŸ’¡ [GPU-{request_id}] ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:100]}...")

    # ìµœì¢… GPU ìƒíƒœ ë¡œê¹…
    final_gpu_status = get_gpu_status()
    logger.info(f"ğŸ–¥ï¸ [GPU-{request_id}] ìƒì„± í›„ GPU ë©”ëª¨ë¦¬: {final_gpu_status['memory_used']:.2f}GB / {final_gpu_status['memory_total']:.2f}GB")

    return response_text.strip(), timings

