        file_content = await file.read()
        if len(file_content) > MAX_UPLOAD_BYTES:
            raise HTTPException(status_code=413, detail=f"íŒŒì¼ í¬ê¸°ê°€ ì œí•œ({MAX_UPLOAD_BYTES} bytes)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤")
        
        file_extension = file.filename.split('.')[-1].lower() if '.' in file.filename else ''
        
        supported_types = ['pdf', 'docx', 'doc', 'txt', 'png', 'jpg', 'jpeg', 'gif', 'bmp']
        if file_extension not in supported_types:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(supported_types)}"
            )
        
        image_types = ['png', 'jpg', 'jpeg', 'gif', 'bmp']
        is_image = file_extension in image_types
        
        conversation_id = get_or_create_conversation(conversation_id)
        
        if is_image:
            image_base64 = base64.b64encode(file_content).decode('utf-8')
            image_data_url = f"data:image/{file_extension};base64,{image_base64}"
            
            request = VisionRequest(
                message=message,
                image_data=image_data_url,
                conversation_id=conversation_id,
                max_tokens=max_tokens
            )
            
            return await analyze_vision(request)
        else:
            file_base64 = base64.b64encode(file_content).decode('utf-8')
            
            request = MultimodalRequest(
                message=message,
                file_data=file_base64,
                file_type=file_extension,
                conversation_id=conversation_id,
                max_tokens=max_tokens
            )
            
            return await multimodal_analysis(request)
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

# === ëŒ€í™” ê´€ë¦¬ ì—”ë“œí¬ì¸íŠ¸ ===
@app.get("/conversations/{conversation_id}")
async def get_conversation_history(conversation_id: str):
    if conversation_id not in active_conversations:
        raise HTTPException(status_code=404, detail="ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return {
        "conversation_id": conversation_id,
        "messages": active_conversations[conversation_id],
        "message_count": len(active_conversations[conversation_id])
    }

@app.delete("/conversations/{conversation_id}")
async def delete_conversation(conversation_id: str):
    if conversation_id not in active_conversations:
        raise HTTPException(status_code=404, detail="ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    del active_conversations[conversation_id]
    return {"message": f"ëŒ€í™” {conversation_id}ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤"}

@app.get("/conversations")
async def list_conversations():
    conversations = []
    for conv_id, messages in active_conversations.items():
        last_message = messages[-1] if messages else None
        conversations.append({
            "conversation_id": conv_id,
            "message_count": len(messages),
            "last_message_time": last_message["timestamp"] if last_message else None,
            "last_message_preview": last_message["content"][:50] + "..." if last_message and len(last_message["content"]) > 50 else last_message["content"] if last_message else None
        })
    
    return {
        "conversations": conversations,
        "total_conversations": len(conversations)
    }

@app.get("/status/detailed")
async def detailed_status():
    gpu_status = get_gpu_status()
    vllm_stats = await get_vllm_stats()
    
    return {
        "server_info": {
            "version": "3.1.0",
            "engine": "vLLM",
            "multimodal_support": "enabled",
            "uptime_seconds": round(time.time() - server_start_time, 2),
            "python_version": "3.10+",
            "pytorch_version": torch.__version__ if torch else "N/A"
        },
        "model_info": {
            "loaded": vllm_engine is not None,
            "name": os.getenv("MODEL_NAME", "N/A"),
            "max_model_len": os.getenv("VLLM_MAX_MODEL_LEN", "4096"),
            "max_num_seqs": os.getenv("VLLM_MAX_NUM_SEQS", "8"),
            "limit_mm_per_prompt": {"image": 1}
        },
        "gpu_info": {
            "available": torch.cuda.is_available() if torch else False,
            "device_count": torch.cuda.device_count() if torch and torch.cuda.is_available() else 0,
            "memory_used_gb": gpu_status["memory_used"],
            "memory_total_gb": gpu_status["memory_total"],
            "memory_usage_percent": round((gpu_status["memory_used"] / gpu_status["memory_total"]) * 100, 2) if gpu_status["memory_total"] > 0 else 0,
            "utilization": float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.85"))
        },
        "vllm_info": vllm_stats,
        "conversation_info": {
            "active_conversations": len(active_conversations),
            "total_messages": sum(len(msgs) for msgs in active_conversations.values())
        }
    }

if __name__ == "__main__":
    host = os.getenv("LLM_HOST", "0.0.0.0")
    port = int(os.getenv("LLM_PORT", "8001"))
    
    print(f"ğŸš€ CloudLLM vLLM ë©€í‹°ëª¨ë‹¬ ì„œë²„ ì‹œì‘")
    print(f"ğŸ“ ì£¼ì†Œ: http://{host}:{port}")
    print(f"ğŸ“š API ë¬¸ì„œ: http://{host}:{port}/docs")
    print(f"âš¡ Engine: vLLM with Multimodal Support")
    print(f"ğŸ”¢ Max Batch Size: {os.getenv('VLLM_MAX_NUM_SEQS', '8')}")
    print(f"ğŸ“ Max Model Length: {os.getenv('VLLM_MAX_MODEL_LEN', '4096')}")
    print(f"ğŸ–¼ï¸ ë©€í‹°ëª¨ë‹¬: ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ + íŒŒì¼ ì§€ì›")
    
    uvicorn.run(
        app, 
        host=host, 
        port=port,
        log_level=os.getenv("LOG_LEVEL", "info").lower(),
        access_log=True
    )